{"text": "Cheat Sheet www.mapdb.org\nMaven\n<dependency>\n    <groupId>org.mapdb</groupId>\n    <artifactId>mapdb</artifactId>\n    <version>[version]</version>\n</dependency>\nMap stored in file\nimport org.mapdb.*;        \n//open (or create) database \nFile file = new File(\u201cdbFileName\u201d);\nDB db = DBMaker\n.newFileDB(file)\n.make(); \n//use map\nMap map = db.getHashMap(\u201cmapName\u201d);\nmap.put(\u201caa\u201d,\u201dbb\u201d);  \n//commit and close database\ndb.commit();\ndb.close();\nIn-memory off-heap Map\n// same as above except different method\nDB db = DBMaker\n.newMemoryDirectDB();\n.make(); \nIn-memory off-heap Queue\n// same as above except different method\nDB db = DBMaker\n.newMemoryDirectDB();\n.make(); \nQueue q = db.getQueue(\u201cq\u201d);Options to make it faster\nDB db = DBMaker\n// all options works with files as well\n  .newMemoryDB();          \n// disable transactions make writes\n// but you may lose data if store crashes\n  .transactionsDisable()    \n// memory mapped files are faster\n// but does not work well on 32bit\n  .mmapFileEnable()        \n// writes done by background thread\n// it has some overhead, so could be slower\n  .asyncWriteEnable()       \n// increase cache size if you have free heap\n// default value is 32K\n  .cacheSize(1000000)     \n  .make(); \nOther DBMaker options\n// encrypt data with password  \n  .encryptionEnable(\u201cpassword\u201d) \n// use fast compression \n  .compressionEnable()   \n// enables CRC32 checksum \n// to protect from data corruption\n  .checksumEnable()       \nCache options\n// It caches deserialized objects on heap.\n// Default cache size is 32,000, increase it\n  .cacheSize(1000000)     \n// enable least-recently-used cache \n  .cacheLRUEnable()\n// Unbounded soft-reference cache\n// use with plenty of free heap\n  .cacheSoftRefEnable() // Hard ref, use if \nheap is larger then store\n  .cacheHardRefEnable()Concurrent transactions\n// By default there is single-global \n// transaction per store. \n// This enables proper transactions \n// with full serializable isolation\nTxMaker txMaker = DBMaker\n.newFileDB(file)\n          .makeTxMaker();\n// open two transactions, with single map\n// both can only see their own changes\nDB tx1 = txMaker.makeTx();\nMap map1 = tx1.getTreeMap(\"map\");\nDB tx2 = txMaker.makeTx();\nMap map2 = tx2.getTreeMap(\"map\");\n//commit and close\ntx1.commit()\ntx2.commit()\ntxMaker.close()\nSnapshots\n// lighter way to get consistent data view\nDB  db = DBMaker\n.newFileDB(file)\n.snapshotEnable()\n.make()\nMap map = db.getHashMap(\u201cmap\u201d);\nmap.put(1,2);\nDB snap = db.snapshot();\nMap mapOld = snap.getHashMap(\u201cmap\u201d);\nmap.put(3,4);   //mapOld still has only 1,2\nsnap.close();   //release resources\n// Third way to ensure consistency is \n// Compare and Swap operation. MapDB \n// has ConcurrentMap and atomic variables.\nMapDB\nCheat Sheet www.mapdb.org\nMaps and Sets\n// Shows how to get all available collections\nDB db = DBMaker\n.newMemoryDirectDB();\n.make(); \n// BTreeMap is good for small sorted keys\nConcurrentNavigableMap treeMap =\ndb.getTreeMap(\u201ctreeMap\u201d);\n// HashMap (aka HTreeMap) is good for\n// larger keys and as a cache \nConcurrentMap hashMap =\ndb.getHashMap(\u201chashMap\u201d);\n// there is also TreeSet and HashSet\nSortedSet treeSet = db.getTreeSet(\u201cts\u201d);\nSet  hashSet = db.getHashSet(\u201chashSet\u201d);\nQueues\n// first-in-first-out queue\nBlockingQueue fifo = db.getQueue(\u201cfifo\u201d);\n// last-in-first-out queue (stack)\nBlockingQueue lifo = db.getStack(\u201clifo\u201d);\n// circular queue with limited size\nBlockingQueue c =\n db.getCircularQueue(\u201ccircular\u201d);\nAtomic records\n// atomically updated records stored in DB\n// Useful for example for sequential IDs.\n// there is Long,  Integer, String \n// and general atomic variable\nAtomic.Long q =db.getAtomicLong(\u201clong\u201d);\nq.set(1999);\nlong id = q.incremendAndGet();Configuring maps\n// create map optimized for large values\nMap<String,String> m = \n   db.createTreeMap(\u201ctreeMap\u201d);\n//serializers are critical for performance\n   .keySerializer(BTreeKeySerializer.STRING) \n// compress large ASCII string values   \n   .valueSerializer(\nnew Serializer.CompressionWrapper(\nSerializer.STRING_ASCII))\n// and store values outside of BTree nodes\n   .valuesOutsideNodesEnable()\n// enable size counter\n   .counterEnable()\n// make BTree nodes larger \n   .nodeSize(120)\n// and finally create map\n   .makeOrGet();\nSecondary indexes\n// create secondary value (1:1 relation)\n// secondary map gets auto updated\nMap<ID, Person> persons \nMap<ID, Branch> branches\nBind.secondaryValue(persons,branches,\n   (person)-> person.getBranch()));\n// create secondary key (index) for age(N:1)\nSortedSet<Fun.Tuple2<Age,ID>> ages\nBind.secondaryKey(persons, ages,\n  (person)-> person.getAge());\n// get all persons of age 32\nfor(ID id: Fun.filter(ages, 32)){\n   Person p = persons.get(id)\n}HTreeMap as a cache\n// Entries are removed if map is too large\n// Off-heap map with max size 16GB\nMap cache = DBMaker\n.newCacheDirect(16)\n// On-disk cache in temp folder \n// with max size 128GB or 1M entries\nDB db = DBMaker\n.newT empFileDB()\n.transactionDisable()\n.closeOnJvmShutdown()\n.deleteFilesAfterClose()\n   .make()\nMap cache = db\n.createHashMap(\"cache\")\n           .expireStoreSize(128)  // GB\n           .expireMaxSize(1000000)\n.make()\nData Pump for faster\nimport\n// Data Pump creates TreeMap and TreeSet \n// in streaming fashion. Import time is linear\n// to number of entries. \nIterator iter = \u2026 iterate over keys..\nMap<K,V> m =  db.createTreeMap(\"map\")\n  .pumpSource(iter, (key)-> key.getValue())\n  .pumpIgnoreDuplicates()\n  .pumpPresort(1000000)\n  .make()\nMapDB\n", "annotations": [[4467, 4470, "SKILL: age"], [3826, 3837, "SKILL: performance"], [2460, 2468, "SKILL: Snapshot"], [4486, 4489, "SKILL: fun"], [5215, 5219, "SKILL: Time"], [2074, 2077, "SKILL: CAN"], [3392, 3393, "SKILL: C"], [2083, 2086, "SKILL: See"], [1847, 1853, "SKILL: Global"], [2612, 2623, "SKILL: Consistency"], [3732, 3738, "SKILL: Values"], [1168, 1173, "SKILL: Value"], [2691, 2697, "SKILL: Atomic"], [3758, 3759, "SKILL: M"], [863, 868, "SKILL: STORE"], [1467, 1473, "SKILL: Caches"], [1174, 1176, "SKILL: IS"], [855, 859, "SKILL: Data"], [306, 310, "SKILL: Make"], [370, 372, "SKILL: AA"], [2579, 2588, "SKILL: Resources"], [1124, 1129, "SKILL: Cache"], [766, 770, "SKILL: WELL"], [1314, 1318, "SKILL: FAST"], [2097, 2104, "SKILL: Changes"], [1319, 1330, "SKILL: Compression"], [3293, 3298, "SKILL: Stack"], [26, 31, "SKILL: Maven"], [5113, 5122, "SKILL: Data Pump"], [757, 762, "SKILL: Files"], [2886, 2890, "SKILL: Keys"], [2605, 2611, "SKILL: Ensure"], [1870, 1873, "SKILL: Per"], [226, 234, "SKILL: Database"], [2741, 2745, "SKILL: Maps"], [317, 320, "SKILL: USE"], [689, 696, "SKILL: Options"], [3188, 3194, "SKILL: Queues"], [3437, 3444, "SKILL: Records"], [1404, 1419, "SKILL: Data Corruption"], [1651, 1660, "SKILL: Reference"], [3681, 3692, "SKILL: Configuring"], [705, 707, "SKILL: IT"], [2789, 2800, "SKILL: Collections"], [439, 445, "SKILL: Memory"], [810, 822, "SKILL: Transactions"], [4223, 4230, "SKILL: Indexes"], [1130, 1134, "SKILL: Size"], [956, 960, "SKILL: Work"], [2643, 2647, "SKILL: SWAP"], [5189, 5198, "SKILL: Streaming"], [668, 669, "SKILL: Q"], [1409, 1419, "SKILL: Corruption"], [2648, 2657, "SKILL: Operation"], [33, 43, "SKILL: Dependency"], [4456, 4461, "SKILL: Index"]]}